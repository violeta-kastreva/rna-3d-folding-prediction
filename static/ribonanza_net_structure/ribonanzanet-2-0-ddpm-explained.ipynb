{"cells":[{"cell_type":"markdown","metadata":{},"source":"## How to Use This Notebook\n\nThe first half of the notebook focuses on the **RibonanzaNet Backbone**. This section is adapted from Moth's notebook (which I used to understand the RibonanzaNet). I have added additional comments and covered some topics in greater detail, such as **Triangular Attention**, to make it more beginner-friendly.\n\nThe second half of the notebook delves into **RibonanzaNet 2.0**, which incorporates a **Diffusion Model** alongside the RibonanzaNet backbone. This too is beginner-friendly, with detailed explanations, comments and diagrams."},{"cell_type":"markdown","metadata":{},"source":"# RibonanzaNet Explained\n\n***\n\nWelcome! üëãüèº\n\nIn this tutorial we will try to understand all the building blocks that compose the `RibonanzaNet` architecture, proposed by @shujun717 et al., which unifies features of `RNAdegformer` and top Kaggle models (from last year's Stanford Ribonanza RNA Folding competition) into a single, self-contained model.\n\nIn the *Table of Contents* you will find links to all the building blocks involved, the block's description, purpose and diagram. Each class is detailed with the different input definitions, tensor shapes and data types to better understand them.\n\nLet's get started!\n\n### Table of Contents\n<div>\n    <li><a href=\"#Introduction\">Introduction</a></li>\n    <li><a href=\"#Ribonanza-Backbone-Architecture\">Ribonanza Backbone Architecture</a></li>\n    <li><a href=\"#2-Outer-Product-Mean\">Outer Product Mean</a></li>\n    <li><a href=\"#4-Relative-Positional-Encoding\">Relative Positional Encoding</a></li>\n    <li><a href=\"#5-Transformer-Encoder\">Transformer Encoder</a></li>\n    <li><a href=\"#f-Triangular-Multiplicative-Module\">Triangle Multiplicative Module</a></li>\n    <li><a href=\"#Bonus-Triangular-Attention\">Triangle Attention</a></li>\n    <li><a href=\"#RibonanzaNet-Backbone\">Ribonanza Net Backbone</a></li>\n    <li><a href=\"#RibonanzaNet-20\">RibonanzaNet 2.0</a></li>\n    <li><a href=\"#Diffusion-Basics\">Diffusion Basics</a></li>\n    <li><a href=\"#RibonanzaNet-20-Architecture\">RibonanzaNet 2.0 Architecture</a></li>\n    <li><a href=\"#Time-Embedder\">Time Embedder</a></li>\n    <li><a href=\"#Embed-Pairwise-Distances\">Embed Pairwise Distances</a></li>\n    <li><a href=\"#Structure-Module\">Structure Module</a></li>\n    <li><a href=\"#Full-Model-Understanding\">Full Model - Training - Inference logic</a></li>\n</div>\n\n\n# Introduction [‚Üë](#top) \n***\n## Some Basics first\n\n1. **Residue/Nucleotide** - A single unit in a chain. For RNA residue is a nucleotide. For example, in the sequence `AUGC`, `A`, `U`, `G` and `C` are residues.\n2. **BPP**: In a sequence, any two residues are called a pair. For some pairs reactivity might be high, for some it may be low. If we know all such pairs for which reactivity is high, it helps us to understand the structure of the RNA, and in some other tasks too. So, it is an important data. For a sequence of length `n`, we represent the BPP as a matrix of size `n*n`. The value at position `(i,j)` in the matrix is the reactivity of the pair of residues `i` and `j`. Pair with high reactivity are called Base Pair. This data is called Base Pair Probability (BPP) data. **TLDR** - We know that this Pair information is important and you'll see this in every other architecture. \n***\n\n[RibonanzaNet][4] was proposed by Shujun He (competition host) et al. in their paper *\"Ribonanza: deep learning of RNA structure through dual crowdsourcing\"*.\n\n**TLDR** - The previous Kaggle competition was to predict the Chemical Reactivity of RNA sequences. They took the top three models from the competition. Below are the tree models:\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/1.png\" width=800 class=\"center\">\n\nIn this tutorial, we will break down the different components that create the `RibonanzaNet`, specially its main layer/block: the `ConvTransformerEncoderLayer`.\n\n<img src=\"https://kaggle-images.s3.us-west-2.amazonaws.com/ribonanza-3d/ribonanza_diagram.png\" width=800 class=\"center\">\n\n### References\n\n- [RibonanzaNet code][1]\n- [How does DeepMind AlphaFold2 work?][2]\n- [AlphaFold v2 Github Repository][3]\n- [Ribonanza paper][4]\n- [AlphaFold2 complementary paper][5]\n\n[1]: https://github.com/Shujun-He/RibonanzaNet/blob/main/Network.py\n[2]: https://borisburkov.net/2021-12-25-1/\n[3]: https://github.com/google-deepmind/alphafold\n[4]: https://www.biorxiv.org/content/10.1101/2024.02.24.581671v1.full.pdf\n[5]: https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_MOESM1_ESM.pdf"},{"cell_type":"markdown","metadata":{},"source":"# Import Libraries [‚Üë](#top) \n\n***"},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-05-05T13:12:57.822016Z","iopub.status.busy":"2025-05-05T13:12:57.821721Z","iopub.status.idle":"2025-05-05T13:13:05.391803Z","shell.execute_reply":"2025-05-05T13:13:05.390645Z","shell.execute_reply.started":"2025-05-05T13:12:57.821989Z"},"trusted":true},"outputs":[],"source":"import math\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport yaml\n\n\nfrom einops import rearrange, repeat, reduce\nfrom einops.layers.torch import Rearrange\nfrom functools import partialmethod\nfrom torch import einsum\nfrom torch.nn.parameter import Parameter\nfrom typing import Any, Dict, List, Literal, Optional, Tuple, Union"},{"cell_type":"markdown","metadata":{},"source":"# Ribonanza Backbone Architecture\n***\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/2.png\" width=400 class=\"center\">\n\nWe will go through the architecture step by step, like the inputs flowing form the start to the end.\n\nFew Conventions: B-> Batch Size, T or S-> Sequence Length, C-> Number of Channels\n\n## Steps:\n\n### 1. **Sequence Embeddings**\n`nn.Embedding(config.ntoken, config.ninp, padding_idx=4)`. Shape (B, T, 256)"},{"cell_type":"markdown","metadata":{},"source":"### 2. **Outer Product Mean**\n\nOne core idea of the `RibonanzaNet` was not to use any precalculated features like BPP but to create a representation of the pairs. If we have a sequence of length `T`, we can create a matrix of size `T*T` to represent the pairs.\n\n- The `OuterProductMean` layer is used to create a matrix of size `T*T` from the sequence embeddings. The output of this layer is a tensor of shape `(B, T, T, 2*C)`.\n\n#### Example:\nI had to understand it using pen and paper. Below is an image of the process. \n\nTaking an example sequence of length 4, with `C=2`. Input is `[4x2]`.\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/3.png\" width=800 class=\"center\">\n\n- Finally, we get a matrix of size `[4x4x4]` with the values of the pairs. The output is a tensor of shape `(B, T, T, 2*C)`.\n- The logic is for the Outer Product Mean. However, we also use linear layers as computing this in full 256 channels is not very efficient.\n\n#### Steps:\n1. **Input Transformation**: `(B, T, 256)` ‚Üí **Linear Layer** ‚Üí `(B, T, 32)`\n2. **Outer Product Mean**: `(B, T, 32)` ‚Üí **Outer Product** ‚Üí `(B, T, T, 32*2)`\n3. **Output Transformation**: `(B, T, T, 64)` ‚Üí **Linear Layer** ‚Üí `(B, T, T, 64)`\n\n---\n\n### Some History:\nThe `OuterProductMean` class was proposed in the paper [Highly accurate protein structure prediction with AlphaFold][1]. It worked differently there, as it was used to update the pair representation through MSA.\n\n[1]: https://www.nature.com/articles/s41586-021-03819-2"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"class Outer_Product_Mean(nn.Module):\n    \"\"\"\n    Outer Product Mean class.\n    :param in_dim: Dimensionality of the input sequence representations (default: 256).\n    :param dim_msa: Intermediate lower-dimensional representation (default: 32).\n    :param pairwise_dim: Final dimensionality of the pairwise output (default: 64).\n    \"\"\"\n    def __init__(\n        self,\n        in_dim: int = 256,\n        dim_msa: int = 32,\n        pairwise_dim: int = 64\n    ):\n        super(Outer_Product_Mean, self).__init__()\n        self.proj_down1 = nn.Linear(in_dim, dim_msa)  # projects the input sequence representation into a lower dimensional space\n        self.proj_down2 = nn.Linear(dim_msa ** 2, pairwise_dim)  # projects the outer product representation (reshaped) to the final pairwise_dim.\n\n    def forward(\n        self,\n        seq_rep: torch.Tensor,  # shape: (batch_size, seq_length, in_dim)\n        pair_rep: torch.Tensor = None  # shape: (batch_size, seq_length, seq_length, pairwise_dim)\n    ):\n        seq_rep = self.proj_down1(seq_rep)  # output shape: (batch_size, seq_length, dim_msa)\n        outer_product = torch.einsum('bid,bjc -> bijcd', seq_rep, seq_rep)  # output shape: (batch_size, seq_length, seq_length, dim_msa, dim_msa)\n        outer_product = rearrange(outer_product, 'b i j c d -> b i j (c d)')  # flattens the last two dimensions: (batch_size, seq_length, seq_length, dim_msa * 2).\n        outer_product = self.proj_down2(outer_product)  # output shape: (batch_size, seq_length, seq_length, pairwise_dim)\n\n        if pair_rep is not None:\n            outer_product = outer_product + pair_rep\n\n        return outer_product "},{"cell_type":"markdown","metadata":{},"source":"### **3. Pairwise Representation**\nBasically the Outer Product Mean layer creates a matrix of size `T*T` to represent the pairs. actually `T*T*2*C`"},{"cell_type":"markdown","metadata":{},"source":"### **4. Relative Positional Encoding**\n\nIn RNA using the absolute positions is not needed, as a sequence `AUGCAU` will have same structure as it's reverse `UACGUA`. We don't need the conventional positional encoding we use in transformers. There is this symmetry.\n \nWhat we need is the **Relative positions of the nucleotides**. \n- For example, in the sequence `AUGCAU`, the relative position of `A` and `U` is `1`, while the relative position of `A` and `C` is `2`. \n- We need to encode the relative positions of the nucleotides.\n- The interactions of these nucleotides are important for the structure of the RNA.\n\n#### **Example**\nLet's take an example of a sequence of length 4. the Pair representation will be of size `4x4`. \n\nWe clip the values less than -8 and more than 8. So in total we have 17 values.\n\nWe do something like \na[i,j] = i - j\n\n\\begin{bmatrix}\n0 & -1 & -2 & -3 \\\\\n1 & 0 & -1 & -2 \\\\\n2 & 1 & 0 & -1 \\\\\n3 & 2 & 1 & 0 \\\\\n\\end{bmatrix}\n\n\nThis is integer level relative positions, we don't add it directly to Pairwise Matrix. What we do is encode them as one-hot vectors. So, we have 17 values, and we can represent them as one-hot vectors of size 17.\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/4.png\" width=400 class=\"center\">\n\n1. `TxT` -> **One Hot Encoding** -> `TxTx17`\n2. Add a **Linear layer** from `17` to pairwise representation size `(64)`. `TxTx17` -> `TxTx64`.\n\n"},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"class relpos(nn.Module):\n    \"\"\"\n    Implements relative positional encoding for sequence-based models.\n    :param dim: (int) The output embedding dimension. Default is 64.\n    \"\"\"\n    \n    def __init__(self, dim: int = 64):\n        super(relpos, self).__init__()\n        self.linear = nn.Linear(33, dim)  # (17,) -> (dim,)\n\n    def forward(self, src: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Computes the relative positional encodings for a given sequence.\n\n        :param src: Input tensor of shape (B, L, D), where:\n            - B: Batch size\n            - L: Sequence length\n            - D: Feature dimension (ignored in this module)\n        :return: Relative positional encoding of shape (L, L, dim)\n        \"\"\"\n        L = src.shape[1]  # Sequence length\n        res_id = torch.arange(L, device=src.device).unsqueeze(0)  # (1, L)\n        \n        device = res_id.device\n        bin_values = torch.arange(-16, 17, device=device)  # (33,)\n\n        d = res_id[:, :, None] - res_id[:, None, :]  # (1, L, L)\n        bdy = torch.tensor(16, device=device)\n\n        # Clipping the values within the range [-16, 16]\n        d = torch.minimum(torch.maximum(-bdy, d), bdy)  # (1, L, L)\n\n        # One-hot encoding of relative positions\n        d_onehot = (d[..., None] == bin_values).float()  # (1, L, L, 33)\n\n        assert d_onehot.sum(dim=-1).min() == 1  # Ensure proper one-hot encoding\n\n        # Linear transformation to embedding space\n        p = self.linear(d_onehot)  # (1, L, L, 33) -> (1, L, L, dim)\n\n        return p.squeeze(0)  # (L, L, dim)\n"},{"cell_type":"markdown","metadata":{},"source":"### **5. Add**\nNow, We add this to the pairwise representation. The pairwise representation will have **some info** about the relative positions of the nucleotides."},{"cell_type":"markdown","metadata":{},"source":"### **5. Transformer Encoder**\nThe `ConvTransformerEncoderLayer` is the main layer of the `RibonanzaNet`.\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/5.png\" width=600 class=\"center\">"},{"cell_type":"markdown","metadata":{},"source":"### **a. 1D Convolution**\n\nSimple 1D convolution.\n\n```python\nself.conv = nn.Conv1d(d_model, d_model, k, padding=k // 2)\nsrc = src + self.conv(src.permute(0, 2, 1)).permute(0, 2, 1)  # Shape: (batch_size, seq_len, d_model)\n```"},{"cell_type":"markdown","metadata":{},"source":"### **b. Linear**\n\nSimple linear layer. This is used to project the pairwise representation to the attention layer. Pairwise representation is of size `TxTx64` and we need to project it to `T x T x num_heads`.\n\nWe are adding this as a bias to the attention layer. So MHA will have size of `T x T x num_heads`."},{"cell_type":"markdown","metadata":{},"source":"### **c. MHA**\n\nMulti head attention\n\nScaled Dot Product Attention woth Multiple heads. This is simply the attention, Nothing special here. Pretty standard.\n\nWe will the the Pairwise represntation from the previous step as the attention bias.\n\n```python\nif mask is not None:\n    attn = attn + mask  # Apply bias mask (B, nhead, L, L)\n```\n\n`mask` is the attention bias, or the pairwise representation."},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"class ScaledDotProductAttention(nn.Module):\n    '''\n    Scaled Dot-Product Attention module, computing attention scores based on query and key similarity.\n    '''\n    \n    def __init__(self, temperature: float, attn_dropout: float = 0.1) -> None:\n        \"\"\"\n        Initializes the Scaled Dot-Product Attention module.\n        \n        :param temperature: Scaling factor for the dot product attention scores.\n        :param attn_dropout: Dropout rate applied to attention weights.\n        \"\"\"\n        super().__init__()\n        self.temperature: float = temperature\n        self.dropout: nn.Dropout = nn.Dropout(attn_dropout)\n\n    def forward(\n        self, \n        q: torch.Tensor,  # (B, nhead, L, d_k) or (B, nH, T, C) T is time, C is num_channels.\n        k: torch.Tensor,  # (B, nhead, L, d_k)\n        v: torch.Tensor,  # (B, nhead, L, d_v)\n        mask: torch.Tensor | None = None,  # (B, 1, L, L) or None\n        attn_mask: torch.Tensor | None = None  # (B, 1, L, L) or None\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass of the Scaled Dot-Product Attention.\n        \n        :param q: Query tensor of shape (B, nhead, L, d_k), where B is batch size, nhead is the number of attention heads,\n                  L is the sequence length, and d_k is the key/query dimension.\n        :param k: Key tensor of shape (B, nhead, L, d_k).\n        :param v: Value tensor of shape (B, nhead, L, d_v), where d_v is the value dimension.\n        :param mask: Optional bias mask tensor of shape (B, 1, L, L), used for causal masking or padding.\n        :param attn_mask: Optional attention mask tensor of shape (B, 1, L, L), where -1 values indicate positions to mask.\n        :return: Tuple containing:\n            - output (torch.Tensor): The result of the attention mechanism, shape (B, nhead, L, d_v).\n            - attn (torch.Tensor): Attention weights after softmax and dropout, shape (B, nhead, L, L).\n        \"\"\"\n        \n        attn = torch.matmul(q, k.transpose(2, 3)) / self.temperature  # (B, nhead, L, L)\n        \n        if mask is not None:\n            attn = attn + mask  # Apply bias mask (B, nhead, L, L)\n        \n        if attn_mask is not None:\n            attn = attn.float().masked_fill(attn_mask == -1, float('-1e-9'))  # Apply attention mask (B, nhead, L, L)\n        \n        attn = self.dropout(F.softmax(attn, dim=-1))  # (B, nhead, L, L)\n        output = torch.matmul(attn, v)  # (B, nhead, L, d_v)\n        \n        return output, attn"},{"cell_type":"markdown","metadata":{},"source":"# MultiHead Attention[‚Üë](#top) \n\n***\n\nThe same process as before can be repeated many times with different Key, Query, and Value projections, forming what is called a multi-head attention. Each head can focus on different projections of the input embeddings. Multihead attention extends self-attention by applying multiple attention mechanisms (or \"heads\") in parallel. Each head learns different attention patterns, which are then combined to produce a more expressive representation.\n\n<img src=\"https://kaggle-images.s3.us-west-2.amazonaws.com/introduction-to-transformers/multihead_attention.png\" width=\"400\" class=\"center\">\n\n### Input shapes\n\nThe inputs `q`, `k`, and `v` (query, key, value) have the following shapes:\n- `q`: [bs, len_q, d_model]\n- `k`: [bs, len_k, d_model]\n- `v`: [bs, len_v, d_model]\n\nWhere:\n- `bs` is the batch size (first dimension)\n- `len_q` is the sequence length of the query\n- `len_k` is the sequence length of the key\n- `len_v` is the sequence length of the value (typically equal to `len_k`)\n- `d_model` is the model's embedding dimension\n\nThe module then projects these inputs into multiple heads:\n- Each head has dimension `d_k` for queries and keys\n- Each head has dimension `d_v` for values\n- There are `n_head` different attention heads\n\nThe attention calculations happen in a shape of `[bs, n_head, len_q, len_k]` and the output has the same dimensionality as the input query: `[bs, len_q, d_model]`.\n\nThis is a standard multi-head attention implementation where vectors are projected into multiple subspaces, attention is calculated separately in each subspace, and then the results are concatenated and projected back to the original dimension.\n\n### Important Note\nThe original RibonanzaNet Model shared initially by Shijun is changed a little bit. We are looking at the new Diffusion model's backbone. Some code is commented out.\n"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"class MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-Head Attention module\n    :param d_model: The number of input features. or C num_channels in input.\n    :param n_head: The number of heads to use.\n    :param d_k: The dimensionality of the keys.\n    :param d_v: The dimensionality of the values.\n    :param dropout: The dropout rate to apply to the attention weights.\n    \"\"\"\n    def __init__(\n        self,\n        d_model: int,\n        n_head: int,\n        d_k: int,\n        d_v: int,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)  # (d_model) -> (n_head * d_k)\n        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)  # (d_model) -> (n_head * d_k)\n        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)  # (d_model) -> (n_head * d_v)\n        # self.fc = nn.Linear(n_head * d_v, d_model, bias=False)  # (n_head * d_v) -> (d_model)\n\n        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n\n        # self.dropout = nn.Dropout(dropout)\n        # self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n\n    def forward(\n        self, \n        q: torch.Tensor,  # Shape: [batch_size, len_q, d_model] # B, T, C\n        k: torch.Tensor,  # Shape: [batch_size, len_k, d_model]\n        v: torch.Tensor,  # Shape: [batch_size, len_v, d_model]\n        mask: Optional[torch.Tensor] = None,  # Optional attention mask\n        src_mask: Optional[torch.Tensor] = None  # Optional source mask\n               \n    ) -> Tuple[torch.Tensor, torch.Tensor]:  # Returns (output, attention)\n\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n        bs, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n    \n        residual = q  # Shape: [bs, len_q, d_model]\n\n        # Linear projections and reshape to multiple heads\n        q = self.w_qs(q).view(bs, len_q, n_head, d_k)  # Shape: [bs, len_q, n_head, d_k]\n        k = self.w_ks(k).view(bs, len_k, n_head, d_k)  # Shape: [bs, len_k, n_head, d_k]\n        v = self.w_vs(v).view(bs, len_v, n_head, d_v)  # Shape: [bs, len_v, n_head, d_v]\n\n        # Transpose for multi-head attention computation\n        q, k, v = (\n            q.transpose(1, 2),  # Shape: [bs, n_head, len_q, d_k]\n            k.transpose(1, 2),  # Shape: [bs, n_head, len_k, d_k]\n            v.transpose(1, 2)\n        )  # Shape: [bs, n_head, len_v, d_v]\n\n        if mask is not None:\n            mask = mask  # Shape remains unchanged\n\n        if src_mask is not None:\n            src_mask=src_mask.clone().unsqueeze(-1).long()\n            src_mask[src_mask==0]=-1\n            src_mask=src_mask.float()\n            #src_mask=src_mask.unsqueeze(-1)#.float()\n            attn_mask=torch.matmul(src_mask,src_mask.permute(0,2,1)).unsqueeze(1).long()\n            q, attn = self.attention(q, k, v, mask=mask,attn_mask=attn_mask)\n        else:\n            q, attn = self.attention(q, k, v, mask=mask)\n        # the output from the attention is self_atten * v. It should not be called q here. Bad confusing variable naming.\n        # Reshape back to original format\n        q = q.transpose(1, 2).contiguous().view(bs, len_q, -1)  # Shape: [bs, len_q, n_head * d_v]\n        # q = self.dropout(self.fc(q))  # Shape: [bs, len_q, d_model]\n        # q += residual  # Shape: [bs, len_q, d_model]\n\n        # q = self.layer_norm(q)  # Shape: [bs, len_q, d_model]\n\n        return q, attn  # Output: [bs, len_q, n_head * d_v], Attention: [bs, n_head, len_q, len_k]\n"},{"cell_type":"markdown","metadata":{},"source":"### **d. Position-wise Feedforward Network**\n\nCheck the code for \n\nSimply a feedforward network with **two linear** layers and a **GELU** activation function. \n```python\n# Position-wise Feedforward according to architecture diagram\nsrc2 = self.linear2(self.dropout(self.activation(self.linear1(src))))  # Shape: (batch_size, seq_len, d_model)\nsrc = src + self.dropout2(src2)\nsrc = self.norm2(src)\n\n```\n"},{"cell_type":"markdown","metadata":{},"source":"### **e. Outer Product Mean**\n\nThe output of MHA is a sequence back to the original size. `BxTxC` or `BxTx256`. \nNow we have to update the pairwise representation also. We want to update those representation using the MHA output. \n\nSimply take the Outer Product Mean\n#### Steps:\n1. **Input Transformation**: `(B, T, 256)` ‚Üí **Linear Layer** ‚Üí `(B, T, 32)`\n2. **Outer Product Mean**: `(B, T, 32)` ‚Üí **Outer Product** ‚Üí `(B, T, T, 32*2)`\n3. **Output Transformation**: `(B, T, T, 64)` ‚Üí **Linear Layer** ‚Üí `(B, T, T, 64)`\n\n\nThen add this to the pairwise representation. This helps information flow from the sequence MHA to the pairwise representation."},{"cell_type":"markdown","metadata":{},"source":"### **f. Triangular Multiplicative Module**\n\nTriangular Update:\n\nAccording to my understanding, we aim to establish relationships between pairs of nucleotides. This is achieved using Pairwise Representation. Each pair is influenced by a third nucleotide. While it is true that all nucleotides influence each other, the inspiration for this approach is rooted in geometry. For any consistent structure, three points in space always follow the Triangle Inequality.\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/7.png\" width=400 class=\"center\">\n\nWe are not explicitly using the Triangle Inequality but are inspired by its concept.\n\n---\n\n### **Think in Terms of Edges, Not Nucleotides**\n\nThe Pairwise Representation can be thought of as directed edge storage, where `a[i, j]` represents the edge from `i` to `j`.\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/6.png\" width=300 class=\"center\">\n\nConsider an edge from `i` to `j`:\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/8.png\" width=300 class=\"center\">\n\nFor this edge, there are two types of edges to consider:\n\n1. **Incoming Edges**: Edges incoming to `i` and `j`.\n2. **Outgoing Edges**: Edges outgoing from `i` and `j`.\n\nThe influence of these edges on the edge `i ‚Üí j` is crucial. Therefore, we update the Pairwise Representation using these edges.\n\n---\n\n### **How Do We Update the Pairwise Representation?**\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/9.png\" width=800 class=\"center\">\n\n> **Note**: Ignore the `x'` in the image. The edge `i ‚Üí j` is the focus for now.\n\n#### **Steps**:\n\n1. Take all the **left edges** and apply a linear layer to them.\n2. Take all the **right edges** and apply a linear layer to them.\n3. Take the **central edge** and apply a linear layer, followed by a sigmoid activation `(0, 1)`. This acts as a **gate**.\n4. Multiply the left and right edges with the gate. This determines how much influence the left and right edges have on the central edge.\n5. Multiply the left and right influences with each other.\n6. Sum the influence of all the edges.\n7. Apply a linear layer to the sum to project it back to the Pairwise Representation size.\n8. Take the **central edge**, apply a linear layer, and then a sigmoid activation `(0, 1)`. This acts as another **gate**.\n9. Multiply the sum (combined influence of left & right edges) with the gate. This determines how much influence the sum has on the central edge.\n10. Finally, add this to the Pairwise Representation at `i, j`. Essentially, we are updating the Pairwise Representation of `i ‚Üí j` using the influence of all the edges.\n\nDo dive into the code to answer whether those two gates are same or not.\n\n---\n\n### **Key Insight**\n\nWe are considering all the triangles with respect to the edge `i ‚Üí j`."},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"def exists(val):\n    return val is not None\n\ndef default(val, d):\n    return val if val is not None else d\n\nclass TriangleMultiplicativeModule(nn.Module):\n    \"\"\"\n    This class is applied to the pairwise residue representations, ensuring that the predicted distances \n    between residues adhere to the triangle inequality principle.\n    \"\"\"\n    def __init__(\n        self,\n        *,\n        dim: int,\n        hidden_dim: Optional[int] = None,\n        mix: str = 'ingoing'\n    ):\n        super().__init__()\n        assert mix in {'ingoing', 'outgoing'}, 'mix must be either ingoing or outgoing'\n\n        hidden_dim = default(hidden_dim, dim)\n        self.norm = nn.LayerNorm(dim)\n\n        self.left_proj = nn.Linear(dim, hidden_dim)\n        self.right_proj = nn.Linear(dim, hidden_dim)\n        self.left_gate = nn.Linear(dim, hidden_dim)\n        self.right_gate = nn.Linear(dim, hidden_dim)\n        self.out_gate = nn.Linear(dim, hidden_dim)\n\n        # Initialize all gating to identity\n        for gate in (self.left_gate, self.right_gate, self.out_gate):\n            nn.init.constant_(gate.weight, 0.)\n            nn.init.constant_(gate.bias, 1.)\n\n        if mix == 'outgoing':\n            self.mix_einsum_eq = '... i k d, ... j k d -> ... i j d'\n        elif mix == 'ingoing':\n            self.mix_einsum_eq = '... k j d, ... k i d -> ... i j d'\n\n        self.to_out_norm = nn.LayerNorm(hidden_dim)\n        self.to_out = nn.Linear(hidden_dim, dim)\n\n    def forward(\n        self,\n        x: torch.Tensor,                  # (batch_size, seq_len, seq_len, dim)\n        src_mask: Optional[torch.Tensor] = None  # (batch_size, seq_len)\n    ) -> torch.Tensor:                    # Output: (batch_size, seq_len, seq_len, dim)\n        if exists(src_mask):\n            src_mask = src_mask.unsqueeze(-1).float()  # (batch_size, seq_len, 1)\n            mask = torch.matmul(src_mask, src_mask.permute(0, 2, 1))  # (batch_size, seq_len, seq_len)\n            mask = rearrange(mask, 'b i j -> b i j ()')  # (batch_size, seq_len, seq_len, 1)\n\n        assert x.shape[1] == x.shape[2], 'feature map must be symmetrical'\n        \n        x = self.norm(x)  # (batch_size, seq_len, seq_len, dim)\n\n        left = self.left_proj(x)  # (batch_size, seq_len, seq_len, hidden_dim)\n        right = self.right_proj(x) # (batch_size, seq_len, seq_len, hidden_dim)\n\n        if exists(src_mask):\n            left = left * mask  # (batch_size, seq_len, seq_len, hidden_dim)\n            right = right * mask # (batch_size, seq_len, seq_len, hidden_dim)\n\n        left_gate = self.left_gate(x).sigmoid()   # (batch_size, seq_len, seq_len, hidden_dim)\n        right_gate = self.right_gate(x).sigmoid() # (batch_size, seq_len, seq_len, hidden_dim)\n        out_gate = self.out_gate(x).sigmoid()     # (batch_size, seq_len, seq_len, hidden_dim)\n\n        left = left * left_gate   # (batch_size, seq_len, seq_len, hidden_dim)\n        right = right * right_gate # (batch_size, seq_len, seq_len, hidden_dim)\n\n        out = einsum(self.mix_einsum_eq, left, right)  # (batch_size, seq_len, seq_len, hidden_dim)\n\n        out = self.to_out_norm(out)  # (batch_size, seq_len, seq_len, hidden_dim)\n        out = out * out_gate         # (batch_size, seq_len, seq_len, hidden_dim)\n        return self.to_out(out)      # (batch_size, seq_len, seq_len, dim)\n"},{"cell_type":"markdown","metadata":{},"source":"### **g. Triangular Update**\n\nSame for Incoming Edges, Column wise in code instead of row wise."},{"cell_type":"markdown","metadata":{},"source":"### **Bonus: Triangular Attention**\n\nThis is not actually used in the RibonanzaNet. It is present but disabled by the original notebook. But since we are here, let's understand it.\n\nSimple attention is to capture relationships between the tokens, here the nucleotides. But we need to capture the relationships between the edges. Each edge is influenced by the other edges or other nucleotides. So, to capture the influence of some other nucleotide on an edge, we need at least 3 nucleotides. Thus the name **Triangular Attention**.\n\nThe AlphaFold paper mentions this, again inspired by the triangle inequality. But for me, the explanation above is better.\n\nPairwise representation is a matrix of size `T*T`, acting as a storage of directed edges. `a[i, j]` is the edge from `i` to `j`.\n\nFor each edge, we have a starting node and an ending node.  \nWe will calculate two attentions:  \n1. Around the starting node  \n2. Around the ending node  \n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/10.png\" width=400 class=\"center\">\n\nBrush up on your basics of attention. It will be needed here.  \n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/11.png\" width=600 class=\"center\">\n\n**Around the starting node:**  \n\nFor understanding, let's just take the one edge `i ‚Üí j` as the Central Edge. We will calculate the attention around the starting node `i`, specifically for the edge `i ‚Üí j`.\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/12.png\" width=600 class=\"center\">\n\nWe have to do the same for each edge. Then this will be called the attention around the starting node.\n\n**What does this mean?**  \n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/13.png\" width=400 class=\"center\">\n\nThis whole process will be done for the Ending node.(Column wise in code)"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"class TriangleAttention(nn.Module):\n    def __init__(\n        self,\n        in_dim: int = 128,\n        dim: int = 32,\n        n_heads: int = 4,\n        wise: Literal['row', 'col'] = 'row'\n    ):\n        \"\"\"\n        Implements Triangle Attention Mechanism.\n        :param in_dim: Input feature dimension.\n        :param dim: Dimension of query, key, and value per head.\n        :param n_heads: Number of attention heads.\n        :param wise: Whether to apply row-wise or column-wise attention.\n        \"\"\"\n        super(TriangleAttention, self).__init__()\n        self.n_heads = n_heads\n        self.wise = wise\n        self.norm = nn.LayerNorm(in_dim)\n        self.to_qkv = nn.Linear(in_dim, dim * 3 * n_heads, bias=False)\n        self.linear_for_pair = nn.Linear(in_dim, n_heads, bias=False)\n        self.to_gate = nn.Sequential(\n            nn.Linear(in_dim, in_dim),\n            nn.Sigmoid()\n        )\n        self.to_out = nn.Linear(n_heads * dim, in_dim)\n\n    def forward(self, z: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for TriangleAttention.\n        :param z: Input tensor of shape (B, I, J, in_dim). I,J are actually T but here we call them I,J. it's actually B,T,T,C.\n        :param src_mask: Source mask of shape (B, I, J).\n        :return: Output tensor of shape (B, I, J, in_dim).\n        \"\"\"\n        # Spawn pair mask\n        src_mask = src_mask.clone()\n        src_mask[src_mask == 0] = -1\n        src_mask = src_mask.unsqueeze(-1).float()  # (B, I, J, 1)\n        attn_mask = torch.matmul(src_mask, src_mask.permute(0, 2, 1))  # (B, I, J, I)\n\n        wise = self.wise\n        z = self.norm(z)  # (B, I, J, in_dim)\n\n        # Compute bias and gate\n        gate = self.to_gate(z)  # [1] (B, I, J, in_dim)\n        b = self.linear_for_pair(z)  # [5] (B, I, J, n_heads) \n\n        # Compute Q, K, V\n        q, k, v = torch.chunk(self.to_qkv(z), 3, -1)  # [2], [3], [4]: each (B, I, J, n_heads * dim)\n        q, k, v = map(lambda x: rearrange(x, 'b i j (h d)->b i j h d', h=self.n_heads), (q, k, v))  \n        # Each: (B, I, J, n_heads, dim)\n        scale = q.size(-1) ** 0.5  # Scalar\n\n        if wise == 'row':\n            eq_attn = 'brihd,brjhd->brijh'\n            eq_multi = 'brijh,brjhd->brihd'\n            b = rearrange(b, 'b i j (r h)->b r i j h', r=1)  # (B, 1, I, J, n_heads)\n            softmax_dim = 3\n            attn_mask = rearrange(attn_mask, 'b i j->b 1 i j 1')  # (B, 1, I, J, 1)\n        elif wise == 'col':\n            eq_attn = 'bilhd,bjlhd->bijlh'\n            eq_multi = 'bijlh,bjlhd->bilhd'\n            b = rearrange(b, 'b i j (l h)->b i j l h', l=1)  # (B, I, J, 1, n_heads)\n            softmax_dim = 2\n            attn_mask = rearrange(attn_mask, 'b i j->b i j 1 1')  # (B, I, J, 1, 1)\n        else:\n            raise ValueError('wise should be col or row!')\n\n        # Compute attention logits\n        logits = (torch.einsum(eq_attn, q, k) / scale + b)  # [6], [7] (B, I, J, I, n_heads) or (B, I, J, J, n_heads)\n        logits = logits.masked_fill(attn_mask == -1, float('-1e-9'))  # Apply mask\n\n        # Compute attention weights\n        attn = logits.softmax(softmax_dim)  # [8] (B, I, J, I, n_heads) or (B, I, J, J, n_heads)\n\n        # Compute attention output\n        out = torch.einsum(eq_multi, attn, v)  # [9] (B, I, J, n_heads, dim)\n        out = gate * rearrange(out, 'b i j h d-> b i j (h d)')  # [10] (B, I, J, in_dim)\n\n        # Final projection\n        z_ = self.to_out(out)  # (B, I, J, in_dim)\n\n        return z_\n"},{"cell_type":"markdown","metadata":{},"source":"### **h. Transition**\n\nNothing special here. Two linear layers.\n\n## Transformer Encoder Block/ ConvTransformer\n\nThe output will be\n1. Sequence Representation: `B x T x 256`\n2. Pairwise Representation: `B x T x T x 64`"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":"class ConvTransformerEncoderLayer(nn.Module):\n    \"\"\"\n    A Transformer Encoder Layer with convolutional enhancements and pairwise feature processing.\n    \"\"\"\n    \n    def __init__(\n        self,\n        d_model: int,\n        nhead: int,\n        dim_feedforward: int,\n        pairwise_dimension: int,\n        use_triangular_attention: bool,\n        dim_msa: int,\n        dropout: float = 0.1,\n        k: int = 3,\n    ):\n        \"\"\"\n        :param d_model: Dimension of the input embeddings\n        :param nhead: Number of attention heads\n        :param dim_feedforward: Hidden layer size in feedforward network\n        :param pairwise_dimension: Dimension of pairwise features\n        :param use_triangular_attention: Whether to use triangular attention modules\n        :param dropout: Dropout rate\n        :param k: Kernel size for the 1D convolution\n        \"\"\"\n        super(ConvTransformerEncoderLayer, self).__init__()\n\n        # === Attention Layers ===\n        self.self_attn = MultiHeadAttention(d_model, nhead, d_model // nhead, d_model // nhead, dropout=dropout)\n\n        # self.linear1 = nn.Linear(d_model, dim_feedforward)\n        # self.dropout = nn.Dropout(dropout)\n        # self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        # === Layer Norms ===\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        # self.norm3 = nn.LayerNorm(d_model)\n        \n        # === Dropout Layers ===\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        # self.dropout3 = nn.Dropout(dropout)\n\n        self.pairwise2heads = nn.Linear(pairwise_dimension, nhead, bias=False)\n        self.pairwise_norm = nn.LayerNorm(pairwise_dimension)\n        self.activation = nn.GELU()\n\n        # self.conv = nn.Conv1d(d_model, d_model, k, padding=k // 2)\n\n        self.triangle_update_out = TriangleMultiplicativeModule(dim=pairwise_dimension, mix='outgoing')\n        self.triangle_update_in = TriangleMultiplicativeModule(dim=pairwise_dimension, mix='ingoing')\n\n        self.pair_dropout_out = DropoutRowwise(dropout)\n        self.pair_dropout_in = DropoutRowwise(dropout)\n\n        self.use_triangular_attention = use_triangular_attention\n        if self.use_triangular_attention:\n            self.triangle_attention_out = TriangleAttention(\n                in_dim=pairwise_dimension,\n                dim=pairwise_dimension // 4,\n                wise='row'\n            )\n            self.triangle_attention_in = TriangleAttention(\n                in_dim=pairwise_dimension,\n                dim=pairwise_dimension // 4,\n                wise='col'\n            )\n\n            self.pair_attention_dropout_out = DropoutRowwise(dropout)\n            self.pair_attention_dropout_in = DropoutColumnwise(dropout)\n\n        self.outer_product_mean=Outer_Product_Mean(in_dim=d_model,dim_msa=dim_msa,pairwise_dim=pairwise_dimension)\n\n        self.pair_transition = nn.Sequential(\n            nn.LayerNorm(pairwise_dimension),\n            nn.Linear(pairwise_dimension, pairwise_dimension * 4),\n            nn.ReLU(inplace=True),\n            nn.Linear(pairwise_dimension * 4, pairwise_dimension)\n        )\n        # Sequence transition is new\n        self.sequence_transititon=nn.Sequential(nn.Linear(d_model,d_model*4),\n                                                nn.ReLU(),\n                                                nn.Linear(d_model*4,d_model))\n\n\n    def forward(\n        self,\n        input\n    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor] | tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass of the ConvTransformerEncoderLayer.\n\n        :param src: Input tensor of shape (batch_size, seq_len, d_model)\n        :param pairwise_features: Pairwise feature tensor of shape (batch_size, seq_len, seq_len, pairwise_dimension)\n        :param src_mask: Optional mask tensor of shape (batch_size, seq_len)\n        :param return_aw: Whether to return attention weights\n        :return: Tuple containing processed src and pairwise_features (and optionally attention weights)\n        \"\"\"\n        src , pairwise_features, src_mask, return_aw= input\n\n        use_gradient_checkpoint=False\n\n        # src = src * src_mask.float().unsqueeze(-1)  # Shape: (batch_size, seq_len, d_model)\n        # res = src  # residual\n        # # 1D convolution\n        # src = src + self.conv(src.permute(0, 2, 1)).permute(0, 2, 1)  # Shape: (batch_size, seq_len, d_model)\n        # src = self.norm3(src)\n\n        # Linear on Pairwise features\n        pairwise_bias = self.pairwise2heads(self.pairwise_norm(pairwise_features)).permute(0, 3, 1, 2) # Shape: (batch_size, n_head, seq_len, seq_len)\n        # MHA + Pairwise mask\n        src2, attention_weights = self.self_attn(src, src, src, mask=pairwise_bias, src_mask=src_mask)  # Shape: (batch_size, seq_len, d_model)\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        \n        # Position-wise Feedforward according to architecture diagram. Or Sequence transition\n        res=src\n        src=self.sequence_transititon(src)\n        src = res + self.dropout2(src)\n        src = self.norm2(src)\n        \n        pairwise_features = pairwise_features + self.outer_product_mean(src)  # Shape: (batch_size, seq_len, seq_len, pairwise_dimension)\n        #Triangular update\n        pairwise_features = pairwise_features + self.pair_dropout_out(self.triangle_update_out(pairwise_features, src_mask))\n        pairwise_features = pairwise_features + self.pair_dropout_in(self.triangle_update_in(pairwise_features, src_mask))\n        \n        if self.use_triangular_attention:\n            pairwise_features = pairwise_features + self.pair_attention_dropout_out(self.triangle_attention_out(pairwise_features, src_mask))\n            pairwise_features = pairwise_features + self.pair_attention_dropout_in(self.triangle_attention_in(pairwise_features, src_mask))\n        \n        pairwise_features = pairwise_features + self.pair_transition(pairwise_features)  # Shape: (batch_size, seq_len, seq_len, pairwise_dimension)\n\n        if return_aw:\n            return src, pairwise_features, attention_weights  # Shapes: (batch_size, seq_len, d_model), (batch_size, seq_len, seq_len, pairwise_dimension), (batch_size, nhead, seq_len, seq_len)\n        else:\n            return src, pairwise_features  # Shapes: (batch_size, seq_len, d_model), (batch_size, seq_len, seq_len, pairwise_dimension)\n"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[2, 10, 10, 64]\n","[2, 1, 10, 64]\n","[2, 10, 10, 64]\n","[2, 1, 10, 64]\n","[2, 10, 10, 64]\n","[2, 1, 10, 64]\n","[2, 10, 10, 64]\n","[2, 10, 1, 64]\n","torch.Size([2, 10, 128])\n"]}],"source":"# generate a dummy input for ConvTransformerEncoderLayer and run the forward pass\n# if __name__ == \"__main__\":\n# Dummy input\nbatch_size = 2\nseq_len = 10\nd_model = 128\ndim_msa = 32\npairwise_dimension = 64\nnhead = 4\ndim_feedforward = 256\nuse_triangular_attention = True\n\n# Create a random input tensor\nsrc = torch.randn(batch_size, seq_len, d_model)\npairwise_features = torch.randn(batch_size, seq_len, seq_len, pairwise_dimension)\nsrc_mask = torch.ones(batch_size, seq_len)\n\n# Create the ConvTransformerEncoderLayer instance\nlayer = ConvTransformerEncoderLayer(d_model, nhead, dim_feedforward, pairwise_dimension, use_triangular_attention, dim_msa)\n\n# Forward pass\noutput = layer((src, pairwise_features, src_mask, False))\nprint(output[0].shape)  # Output shape: (batch_size, seq_len, d_model)"},{"cell_type":"markdown","metadata":{},"source":"### **7. Reactivities Head**\n\nSimply a Feedforward network, take the sequence representation and project it to the output size, whatever you want to predict. The important thing is the backbone above. \n\nThey trained the model on the reactivities data, Secondary structure data, and some other things too, this worked well.\n\n## **Some Important Helper modules**"},{"cell_type":"markdown","metadata":{},"source":"### Dropout\n\n***\n\nFor the Pairwise Representation, we need to use dropout differently, when we are performing row level operations we need to use dropout on the rows. When we are performing column level operations, we need to use dropout on the columns."},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"class Dropout(nn.Module):\n    \"\"\"\n    Implementation of dropout with the ability to share the dropout mask\n    along a particular dimension.\n\n    If not in training mode, this module computes the identity function.\n    \"\"\"\n\n    def __init__(self, r: float, batch_dim: Union[int, List[int]]):\n        \"\"\"\n        Args:\n            r:\n                Dropout rate\n            batch_dim:\n                Dimension(s) along which the dropout mask is shared\n        \"\"\"\n        super(Dropout, self).__init__()\n\n        self.r = r\n        if type(batch_dim) == int:\n            batch_dim = [batch_dim]\n        self.batch_dim = batch_dim\n        self.dropout = nn.Dropout(self.r)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x:\n                Tensor to which dropout is applied. Can have any shape\n                compatible with self.batch_dim\n        \"\"\"\n        shape = list(x.shape)\n        print(shape)\n        if self.batch_dim is not None:\n            for bd in self.batch_dim:\n                shape[bd] = 1\n        print(shape)\n        mask = x.new_ones(shape)\n        mask = self.dropout(mask)\n        x = x * mask\n        return x\n\n\nclass DropoutRowwise(Dropout):\n    \"\"\"\n    Convenience class for rowwise dropout as described in subsection\n    1.11.6.\n    \"\"\"\n\n    __init__ = partialmethod(Dropout.__init__, batch_dim=-3)\n\n\nclass DropoutColumnwise(Dropout):\n    \"\"\"\n    Convenience class for columnwise dropout as described in subsection\n    1.11.6.\n    \"\"\"\n\n    __init__ = partialmethod(Dropout.__init__, batch_dim=-2)"},{"cell_type":"markdown","metadata":{},"source":"### Utils[‚Üë](#top) "},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":"class Config:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n        self.entries=entries\n\n    def print(self):\n        print(self.entries)\n        \n\ndef default(val: Any, d: Any) -> Any:\n    \"\"\"\n    Returns `val` if it is not None, otherwise returns the default value `d`.\n    :param val: The primary value.\n    :param d: The default value to return if `val` is None.\n    :return: `val` if it is not None, otherwise `d`.\n    \"\"\"\n    return val if exists(val) else d\n\n\ndef exists(val: Any) -> bool:\n    \"\"\"\n    Checks whether a given value is not None.\n    :param val: The value to check.\n    :return: True if `val` is not None, otherwise False.\n    \"\"\"\n    return val is not None\n\n\ndef init_weights(m: torch.nn.Module) -> None:\n    \"\"\"\n    Initializes the weights of a given module if it is an instance of `torch.nn.Linear`. \n    Currently, the function does not apply any initialization but has commented-out \n    Xavier initialization methods.\n    :param m: The module to initialize, expected to be a `torch.nn.Linear` instance.\n    :return: None\n    \"\"\"\n    if m is not None and isinstance(m, nn.Linear):\n        pass\n\n\ndef load_config_from_yaml(file_path):\n    \"\"\"Load YAML file\"\"\"\n    with open(file_path, 'r') as file:\n        config = yaml.safe_load(file)\n    return Config(**config)\n\n\ndef sep():\n    print(\"‚Äî\"*100)\n    \ndef recursive_linear_init(m,scale_factor):\n    for child_name, child in m.named_modules():\n        if 'gate' not in child_name:\n            custom_weight_init(child,scale_factor)\n            \ndef custom_weight_init(m, scale_factor):\n    if isinstance(m, nn.Linear):\n        d_model = m.in_features  # Set d_model to the input dimension of the linear layer\n        upper = 1.0 / (d_model ** 0.5) * scale_factor\n        lower = -1.0 / (d_model ** 0.5) * scale_factor\n        torch.nn.init.uniform_(m.weight, lower, upper)\n        if m.bias is not None:\n            torch.nn.init.zeros_(m.bias)"},{"cell_type":"markdown","metadata":{},"source":"# RibonanzaNet Backbone"},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":"import torch.utils.checkpoint as checkpoint\n\nclass RibonanzaNet(nn.Module):\n\n    #def __init__(self, ntoken=5, nclass=1, ninp=512, nhead=8, nlayers=9, kmers=9, dropout=0):\n    def __init__(self, config):\n\n        super(RibonanzaNet, self).__init__()\n        self.config=config\n        nhid=config.ninp*4\n        self._tied_weights_keys = [] #avoids AttributeError: 'RibonanzaNet' object has no attribute '_tied_weights_keys'\n\n        self.transformer_encoder = []\n        print(f\"constructing {config.nlayers} ConvTransformerEncoderLayers\")\n        for i in range(config.nlayers):\n            if i!= config.nlayers-1:\n                k=5\n            else:\n                k=1\n            #print(k)\n            self.transformer_encoder.append(ConvTransformerEncoderLayer(d_model = config.ninp, nhead = config.nhead,\n                                                                        dim_feedforward = nhid, \n                                                                        pairwise_dimension= config.pairwise_dimension,\n                                                                        use_triangular_attention=config.use_triangular_attention,\n                                                                        dim_msa=config.dim_msa,\n                                                                        dropout = config.dropout, k=k))\n        self.transformer_encoder= nn.ModuleList(self.transformer_encoder)\n        \n        for i,layer in enumerate(self.transformer_encoder):\n            scale_factor=1/(i+1)**0.5\n            #scale_factor=i+1\n            #scale_factor=0\n            recursive_linear_init(layer,scale_factor)\n        \n        self.encoder = nn.Embedding(config.ntoken, config.ninp, padding_idx=4)\n        self.decoder = nn.Linear(config.ninp,config.nclass)\n        recursive_linear_init(self.decoder,scale_factor)\n        \n        self.outer_product_mean=Outer_Product_Mean(in_dim=config.ninp,dim_msa=config.dim_msa,pairwise_dim=config.pairwise_dimension)\n        self.pos_encoder=relpos(config.pairwise_dimension)\n        self.use_gradient_checkpoint=False\n        \n    def custom(self, module):\n        def custom_forward(*inputs):\n            inputs = module(inputs[0])\n            return inputs\n        return custom_forward\n\n    def forward(self, src,src_mask=None,return_aw=False):\n        B,L=src.shape\n        src = src\n        src = self.encoder(src).reshape(B,L,-1)\n        \n        #spawn outer product\n        # outer_product = torch.einsum('bid,bjc -> bijcd', src, src)\n        # outer_product = rearrange(outer_product, 'b i j c d -> b i j (c d)')\n        # print(outer_product.shape)\n        pairwise_features=self.outer_product_mean(src)\n        pairwise_features=pairwise_features+self.pos_encoder(src)\n        # print(pairwise_features.shape)\n        # exit()\n\n        attention_weights=[]\n        for i,layer in enumerate(self.transformer_encoder):\n            src,pairwise_features=layer(src, pairwise_features, src_mask,return_aw=return_aw,use_gradient_checkpoint=self.use_gradient_checkpoint)\n\n        output = self.decoder(src).squeeze(-1)+pairwise_features.mean()*0\n\n        if return_aw:\n            return output, attention_weights\n        else:\n            return output\n        \n    def get_embeddings(self, src,src_mask=None,return_aw=False):\n        B,L=src.shape\n        src = src\n        src = self.encoder(src).reshape(B,L,-1)\n        \n        #spawn outer product\n        # outer_product = torch.einsum('bid,bjc -> bijcd', src, src)\n        # outer_product = rearrange(outer_product, 'b i j c d -> b i j (c d)')\n        # print(outer_product.shape)\n        if self.use_gradient_checkpoint:\n            #print(\"using grad checkpointing\")\n            pairwise_features=checkpoint.checkpoint(self.custom(self.outer_product_mean), src)\n            pairwise_features=pairwise_features+self.pos_encoder(src)\n        else:\n            pairwise_features=self.outer_product_mean(src)\n            pairwise_features=pairwise_features+self.pos_encoder(src)\n        # print(pairwise_features.shape)\n        # exit()\n\n        attention_weights=[]\n        for i,layer in enumerate(self.transformer_encoder):\n            # src,pairwise_features=layer(src, pairwise_features, src_mask,return_aw=return_aw,use_gradient_checkpoint=self.use_gradient_checkpoint)\n            src,pairwise_features=checkpoint.checkpoint(self.custom(layer), [src, pairwise_features, src_mask, return_aw],use_reentrant=False)#print(src.shape)\n        #output = self.decoder(src).squeeze(-1)+pairwise_features.mean()*0\n\n\n        return src, pairwise_features"},{"cell_type":"markdown","metadata":{},"source":"We have done till the backbone of the RibonanzaNet. Now we will look at the RibonanzaNet 2.0. Some changes in the architecture and some new modules.\n\n# RibonanzaNet 2.0\n***"},{"cell_type":"markdown","metadata":{},"source":"# Diffusion Basics\n\n***\n\nA diffusion model is a generative model that learns to reverse a gradual noising process, transforming random noise into structured data such as images or molecular structures. It works by simulating a Markov chain that progressively adds noise to data and then learns to denoise it step by step.\n\n---\n\n## Training Process\n\nWe decide a number of steps `T` and a noise schedule $\\beta_t \\in [0, 1]$ for each step $t$. And $\\alpha_t = 1 - \\beta_t$. We define $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$.\n\nWe need this $\\bar{\\alpha}_t$ to calculate the forward diffusion process.\n\n**Forward Diffusion Process Equation:**\n\n$$\nx_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n$$\n\nWhere:\n\n- $x_0$ is the original data (e.g., here the correct co-ordinates).\n- $x_t$ is the noisy version of the data at step $t$.\n- $\\epsilon$ is the noise added to the data.\n\nNow, we create a model that takes input as $x_t, t$ and predicts the noise $\\epsilon$.\n\nWe use a simple MSE loss function to train the model, using the actual noise $\\epsilon$ and the predicted noise $\\hat{\\epsilon}$.\n\n---\n\n## Inference Process\n\n- Start with a random noise $x_T$.\n- At each step $t$ (from $T$ down to $1$):\n    - Use the trained model to predict the noise $\\epsilon$ given $x_t$ and $t$.\n    - Compute $x_{t-1}$ using the reverse diffusion process, removing the predicted noise from $x_t$.\n- Repeat this process iteratively until reaching $x_0$.\n- The final $x_0$ is the generated data (e.g., predicted coordinates).\n---\n\nKeep in mind these terms:\n- $\\epsilon$ is the noise added to the data.\n- $x_t$ is the noisy version of the data at step $t$.\n- $x_0$ is the original data (e.g., here the correct co-ordinates).\n- $t$ is the time step.\n- $\\beta_t$ is the noise schedule for each step $t$.\n- $\\alpha_t = 1- \\beta_t$.\n- $\\bar{\\alpha}_t$ is the cumulative product of $\\alpha_t$ up to time step $t$.\n- Forward diffusion formula.\n\nThese all are the components of the diffusion model."},{"cell_type":"markdown","metadata":{},"source":"# RibonanzaNet 2.0 Architecture\n\n<img src=\"https://raw.githubusercontent.com/siddhantoon/storage/main/RibonanzaNet-2.0-DDPM-explained/14.png\" width=900 class=\"center\">"},{"cell_type":"markdown","metadata":{},"source":"### Time Embedder\nSinusoidal Positional embeddings for a `768` dimensional vector.\nTake an input t (integer timestep of diffusion). Create `768` dim Positional encoded vector for just the single timestep `t`.\nStandard positional encoding of transformers but instead of a sequence just a timestep `t`.\n\nIn standard transformers, the t is the sequence element. But here we are using `t` as diffusion timestep, so the positional encoding will be same for every element in the sequence. Don't confuse it with the positional encoding of the sequence in transformers."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"class SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb"},{"cell_type":"markdown","metadata":{},"source":"### Embed Pairwise Distances\n\nWe have $x_t$ as the input. These are xyz co-ordinates of the nucleotides. Simply take the pairwise distances of the co-ordinates.\n\n`a[i,j]` = euclidean distance between `i-th` and `j-th` nucleotide.\n\nThis is a matrix of size `T*T`. Add this to the pairwise representation. Tensor broadcasting will do the work."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Don't run this, it's part of final class we'll make\ndef embed_pair_distance(self,inputs):\n    pairwise_features,xyz=inputs\n    distance_matrix=xyz[:,None,:,:]-xyz[:,:,None,:]\n    distance_matrix=(distance_matrix**2).sum(-1).clip(2,37**2).sqrt()\n    distance_matrix=distance_matrix[:,:,:,None]\n    pairwise_features=pairwise_features+self.distance2pairwise(distance_matrix)\n\n    return pairwise_features"},{"cell_type":"markdown","metadata":{},"source":"## Structure Module\n\nThe diagram is self explanatory.\n\n1. We take diffusion timestep `t` and create a time representation.\n2. Sequence features(SF) from the backbone. and apply linear layer to it. 256  -> 768.\n3. x_t are xyz co-ordinates at timestep `t`. Apply linear layer to it. 3 -> 768.\n4. Combine the above to create a single `tgt` vector, having info about the Sequence features  timestep, and xyz co-ordinates.\n5. Take the Pairwise features(PF) and embed xyz distances into it.\n\nThe tgt and PF are the inputs to the Structure Module. It's a much Simpler block similar to the above Transformer block in Ribonanza.\n\nUses the Sequence features for attention and the pairwise features as attention bias. \nThen applies a linear layer.\n\nOutput feature is enriched with the sequence features and the pairwise features."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"class SimpleStructureModule(nn.Module):\n\n    def __init__(self, d_model, nhead, \n                 dim_feedforward, pairwise_dimension, dropout=0.1,\n                 ):\n        super(SimpleStructureModule, self).__init__()\n        #self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.self_attn = MultiHeadAttention(d_model, nhead, d_model//nhead, d_model//nhead, dropout=dropout)\n        #self.cross_attn = MultiHeadAttention(d_model, nhead, d_model//nhead, d_model//nhead, dropout=dropout)\n\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.pairwise2heads=nn.Linear(pairwise_dimension,nhead,bias=False)\n        self.pairwise_norm=nn.LayerNorm(pairwise_dimension)\n\n        #self.distance2heads=nn.Linear(1,nhead,bias=False)\n        #self.pairwise_norm=nn.LayerNorm(pairwise_dimension)\n\n        self.activation = nn.GELU()\n\n        \n    def custom(self, module):\n        def custom_forward(*inputs):\n            inputs = module(*inputs)\n            return inputs\n        return custom_forward\n\n    def forward(self, input):\n        tgt , src,  pairwise_features, pred_t, src_mask = input\n        \n        #src = src*src_mask.float().unsqueeze(-1)\n\n        # MHA + Pairwise mask\n        pairwise_bias=self.pairwise2heads(self.pairwise_norm(pairwise_features)).permute(0,3,1,2)\n        #print(pairwise_bias.shape,distance_bias.shape)\n        #pairwise_bias=pairwise_bias+distance_bias\n        res=tgt\n        tgt,attention_weights = self.self_attn(tgt, tgt, tgt, mask=pairwise_bias, src_mask=src_mask)\n        tgt = res + self.dropout1(tgt)\n        tgt = self.norm1(tgt)\n\n        # print(tgt.shape,src.shape)\n        # exit()\n\n        # FeedForward network\n        res=tgt\n        tgt = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        tgt = res + self.dropout2(tgt)\n        tgt = self.norm2(tgt)\n        return tgt"},{"cell_type":"markdown","metadata":{},"source":"## Full Model Understanding\n\nThe task of the model is to predict the noise in xyz co-ordinates.\n\n\nDuring training, we will do the forward diffusion process. \n\nDuring Inference we will do the reverse diffusion process.\n"},{"cell_type":"markdown","metadata":{},"source":"**Training**\n\n1. Take the xyz co-ordinates of the nucleotides. $x_0$.\n2. Take a random timestep `t` from `1` to `T`.\n3. Run the forward diffusion process to get $x_t$. We have the noise $\\epsilon$.\n4. Run the model to predict the noise $\\hat{\\epsilon}$.\n5. Calculate the loss using MSE between $\\epsilon$ and $\\hat{\\epsilon}$.\n6. Another thing. We apply a Linear layer to the Pairwise representation(calling this distogram) then calculate loss between this and the distance matrix. Objective: to make the pairwise representation as close to the distance matrix as possible. Intuition IDK. Have asked the authors [here](https://www.kaggle.com/code/shujun717/ribonanzanet2-ddpm-training/comments#3204706)\n7. The total loss is weighted sum of the two losses. denoising loss + 0.2 * distogram loss.\n\nThe full training notebook is present [here](https://www.kaggle.com/code/shujun717/ribonanzanet2-ddpm-training/notebook)."},{"cell_type":"markdown","metadata":{},"source":"**Inference**\n\n1. Start with a random noise $x_T$.\n2. At each step $t$ (from $T$ down to $1$):\n    - Use the trained model to predict the noise $\\epsilon$ given $x_t$ and $t$.\n    - Compute $x_{t-1}$ using the reverse diffusion process, removing the predicted noise from $x_t$.\n3. Repeat this process iteratively until reaching $x_0$.\n4. The final $x_0$ is the generated data (e.g., predicted coordinates).\n\nThe full inference notebook is present [here](https://www.kaggle.com/code/shujun717/ribonanzanet2-ddpm-inference/notebook).\n\nBelow is the code for the model. We load the backbone and inherit the class. Then we add the newer parts to it. Structure Module, Time Embedder, Embed Pairwise Distances, adapter, distogram predictor, noising & denoising methods."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"class SinusoidalPosEmb(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        device = x.device\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n        emb = x[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n        return emb\n\nclass finetuned_RibonanzaNet(RibonanzaNet):\n    def __init__(self, rnet_config, config, pretrained=False):\n        rnet_config.dropout=0.1\n        rnet_config.use_grad_checkpoint=True\n        super(finetuned_RibonanzaNet, self).__init__(rnet_config)\n        if pretrained:\n            self.load_state_dict(torch.load(config.pretrained_weight_path,map_location='cpu'))\n        # self.ct_predictor=nn.Sequential(nn.Linear(64,256),\n        #                                 nn.ReLU(),\n        #                                 nn.Linear(256,64),\n        #                                 nn.ReLU(),\n        #                                 nn.Linear(64,1)) \n        self.dropout=nn.Dropout(0.0)\n\n        decoder_dim=config.decoder_dim\n        self.structure_module=[\n            SimpleStructureModule(\n                d_model=decoder_dim, \n                nhead=config.decoder_nhead, \n                dim_feedforward=decoder_dim*4, \n                pairwise_dimension=rnet_config.pairwise_dimension, \n                dropout=0.0) \n            for i in range(config.decoder_num_layers)\n        ]\n        self.structure_module=nn.ModuleList(self.structure_module)\n\n        self.xyz_embedder=nn.Linear(3,decoder_dim)\n        self.xyz_norm=nn.LayerNorm(decoder_dim)\n        self.xyz_predictor=nn.Linear(decoder_dim,3)\n        \n        self.adaptor=nn.Sequential(nn.Linear(rnet_config.ninp,decoder_dim),nn.LayerNorm(decoder_dim))\n\n        self.distogram_predictor=nn.Sequential(nn.LayerNorm(rnet_config.pairwise_dimension),\n                                                nn.Linear(rnet_config.pairwise_dimension,40))\n\n        self.time_embedder=SinusoidalPosEmb(decoder_dim)\n\n        self.time_mlp=nn.Sequential(nn.Linear(decoder_dim,decoder_dim),\n                                    nn.ReLU(),  \n                                    nn.Linear(decoder_dim,decoder_dim))\n        self.time_norm=nn.LayerNorm(decoder_dim)\n\n        self.distance2pairwise=nn.Linear(1,rnet_config.pairwise_dimension,bias=False)\n\n        self.pair_mlp=nn.Sequential(nn.Linear(rnet_config.pairwise_dimension,rnet_config.pairwise_dimension),\n                                    nn.ReLU(),\n                                    nn.Linear(rnet_config.pairwise_dimension,rnet_config.pairwise_dimension))\n\n\n        #hyperparameters for diffusion\n        self.n_times = config.n_times\n\n        #self.model = model\n        \n        # define linear variance schedule(betas)\n        beta_1, beta_T = config.beta_min, config.beta_max\n        betas = torch.linspace(start=beta_1, end=beta_T, steps=config.n_times)#.to(device) # follows DDPM paper\n        self.sqrt_betas = torch.sqrt(betas)\n                                     \n        # define alpha for forward diffusion kernel\n        self.alphas = 1 - betas\n        self.sqrt_alphas = torch.sqrt(self.alphas)\n        alpha_bars = torch.cumprod(self.alphas, dim=0)\n        self.sqrt_one_minus_alpha_bars = torch.sqrt(1-alpha_bars)\n        self.sqrt_alpha_bars = torch.sqrt(alpha_bars)\n\n        self.data_std=config.data_std\n\n\n    def custom(self, module):\n        def custom_forward(*inputs):\n            inputs = module(*inputs)\n            return inputs\n        return custom_forward\n    \n    def embed_pair_distance(self,inputs):\n        pairwise_features,xyz=inputs\n        distance_matrix=xyz[:,None,:,:]-xyz[:,:,None,:]\n        distance_matrix=(distance_matrix**2).sum(-1).clip(2,37**2).sqrt()\n        distance_matrix=distance_matrix[:,:,:,None]\n        pairwise_features=pairwise_features+self.distance2pairwise(distance_matrix)\n\n        return pairwise_features\n\n    def forward(self,src,xyz,t):\n        \n        #with torch.no_grad():\n        sequence_features, pairwise_features=self.get_embeddings(src, torch.ones_like(src).long().to(src.device))\n        \n        distogram=self.distogram_predictor(pairwise_features)\n\n        sequence_features=self.adaptor(sequence_features)\n\n        decoder_batch_size=xyz.shape[0]\n        sequence_features=sequence_features.repeat(decoder_batch_size,1,1)\n        \n\n        pairwise_features=pairwise_features.expand(decoder_batch_size,-1,-1,-1)\n\n        pairwise_features= checkpoint.checkpoint(self.custom(self.embed_pair_distance), [pairwise_features,xyz],use_reentrant=False)\n\n        time_embed=self.time_embedder(t).unsqueeze(1)\n        tgt=self.xyz_norm(sequence_features+self.xyz_embedder(xyz)+time_embed)\n\n        tgt=self.time_norm(tgt+self.time_mlp(tgt))\n\n        for layer in self.structure_module:\n            #tgt=layer([tgt, sequence_features,pairwise_features,xyz,None])\n            tgt=checkpoint.checkpoint(self.custom(layer),\n            [tgt, sequence_features,pairwise_features,xyz,None],\n            use_reentrant=False)\n            # xyz=xyz+self.xyz_predictor(sequence_features).squeeze(0)\n            # xyzs.append(xyz)\n            #print(sequence_features.shape)\n        \n        xyz=self.xyz_predictor(tgt).squeeze(0)\n        #.squeeze(0)\n\n        return xyz, distogram\n    \n\n    def denoise(self,sequence_features,pairwise_features,xyz,t):\n        # t is tensor([4, 4, 4, 4, 4]). if the t-th step is 4-th step. and number of samples is 5.\n        decoder_batch_size=xyz.shape[0]\n        sequence_features=sequence_features.expand(decoder_batch_size,-1,-1)\n        pairwise_features=pairwise_features.expand(decoder_batch_size,-1,-1,-1)\n\n        pairwise_features=self.embed_pair_distance([pairwise_features,xyz])\n\n        sequence_features=self.adaptor(sequence_features) # B,T,768\n        time_embed=self.time_embedder(t).unsqueeze(1)\n        tgt=self.xyz_norm(sequence_features+self.xyz_embedder(xyz)+time_embed)\n        tgt=self.time_norm(tgt+self.time_mlp(tgt))\n        #xyz_batch_size=xyz.shape[0]\n        \n\n\n        for layer in self.structure_module:\n            tgt=layer([tgt, sequence_features,pairwise_features,xyz,None])\n            # xyz=xyz+self.xyz_predictor(sequence_features).squeeze(0)\n            # xyzs.append(xyz)\n            #print(sequence_features.shape)\n        xyz=self.xyz_predictor(tgt).squeeze(0)\n        # print(xyz.shape)\n        # exit()\n        return xyz\n\n\n    def extract(self, a, t, x_shape):\n        \"\"\"\n            from lucidrains' implementation\n                https://github.com/lucidrains/denoising-diffusion-pytorch/blob/beb2f2d8dd9b4f2bd5be4719f37082fe061ee450/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L376\n        \"\"\"\n        b, *_ = t.shape\n        out = a.gather(-1, t)\n        return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n    \n    def scale_to_minus_one_to_one(self, x):\n        # according to the DDPMs paper, normalization seems to be crucial to train reverse process network\n        return x * 2 - 1\n    \n    def reverse_scale_to_zero_to_one(self, x):\n        return (x + 1) * 0.5\n    \n    def make_noisy(self, x_zeros, t): \n        # assume we get raw data, so center and scale by 35\n        x_zeros = x_zeros - torch.nanmean(x_zeros,1,keepdim=True)\n        x_zeros = x_zeros/self.data_std\n        #rotate randomly\n        x_zeros = random_rotation_point_cloud_torch_batch(x_zeros)\n\n\n        # perturb x_0 into x_t (i.e., take x_0 samples into forward diffusion kernels)\n        epsilon = torch.randn_like(x_zeros).to(x_zeros.device)\n        \n        sqrt_alpha_bar = self.extract(self.sqrt_alpha_bars.to(x_zeros.device), t, x_zeros.shape)\n        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars.to(x_zeros.device), t, x_zeros.shape)\n        \n        # Let's make noisy sample!: i.e., Forward process with fixed variance schedule\n        #      i.e., sqrt(alpha_bar_t) * x_zero + sqrt(1-alpha_bar_t) * epsilon\n        noisy_sample = x_zeros * sqrt_alpha_bar + epsilon * sqrt_one_minus_alpha_bar\n    \n        return noisy_sample.detach(), epsilon\n    \n    \n    # def forward(self, x_zeros):\n    #     x_zeros = self.scale_to_minus_one_to_one(x_zeros)\n        \n    #     B, _, _, _ = x_zeros.shape\n        \n    #     # (1) randomly choose diffusion time-step\n    #     t = torch.randint(low=0, high=self.n_times, size=(B,)).long().to(x_zeros.device)\n        \n    #     # (2) forward diffusion process: perturb x_zeros with fixed variance schedule\n    #     perturbed_images, epsilon = self.make_noisy(x_zeros, t)\n        \n    #     # (3) predict epsilon(noise) given perturbed data at diffusion-timestep t.\n    #     pred_epsilon = self.model(perturbed_images, t)\n        \n    #     return perturbed_images, epsilon, pred_epsilon\n    \n    \n    def denoise_at_t(self, x_t, sequence_features, pairwise_features, timestep, t):\n        B, _, _ = x_t.shape\n        if t > 1:\n            z = torch.randn_like(x_t).to(sequence_features.device)\n        else:\n            z = torch.zeros_like(x_t).to(sequence_features.device)\n        \n        # at inference, we use predicted noise(epsilon) to restore perturbed data sample.\n        epsilon_pred = self.denoise(sequence_features, pairwise_features, x_t, timestep)\n        \n        alpha = self.extract(self.alphas.to(x_t.device), timestep, x_t.shape)\n        sqrt_alpha = self.extract(self.sqrt_alphas.to(x_t.device), timestep, x_t.shape)\n        sqrt_one_minus_alpha_bar = self.extract(self.sqrt_one_minus_alpha_bars.to(x_t.device), timestep, x_t.shape)\n        sqrt_beta = self.extract(self.sqrt_betas.to(x_t.device), timestep, x_t.shape)\n        \n        # denoise at time t, utilizing predicted noise\n        x_t_minus_1 = 1 / sqrt_alpha * (x_t - (1-alpha)/sqrt_one_minus_alpha_bar*epsilon_pred) + sqrt_beta*z\n        \n        return x_t_minus_1#.clamp(-1., 1)\n                \n    def sample(self, src, N):\n        \"\"\"_summary_\n\n        Args:\n            src (_type_): SEQUENCE DIRECTLY FROM DATA 'augc'\n            N (_type_): num of samples to generate\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        # start from random noise vector, NxLx3\n        x_t = torch.randn((N, src.shape[1], 3)).to(src.device) #x_T generate from here.\n        \n        # autoregressively denoise from x_T to x_0\n        #     i.e., generate image from noise, x_T\n\n        #first get conditioning #RibonanzaNet Backbone\n        sequence_features, pairwise_features=self.get_embeddings(src, torch.ones_like(src).long().to(src.device))\n        # sequence_features=sequence_features.expand(N,-1,-1)\n        # pairwise_features=pairwise_features.expand(N,-1,-1,-1)\n        distogram=self.distogram_predictor(pairwise_features).squeeze() # B,T,T,40\n        distogram=distogram.squeeze()[:,:,2:40]*torch.arange(2,40).float().cuda() # \n        distogram=distogram.sum(-1)  # T,T -> Or seq,seq\n\n        for t in range(self.n_times-1, -1, -1): #T to .... 0\n            timestep = torch.tensor([t]).repeat_interleave(N, dim=0).long().to(src.device)\n            x_t = self.denoise_at_t(x_t, sequence_features, pairwise_features, timestep, t)\n        \n        # denormalize x_0 into 0 ~ 1 ranged values.\n        #x_0 = self.reverse_scale_to_zero_to_one(x_t)\n        x_0 = x_t * self.data_std\n        return x_0, distogram\n\n\n\n\nclass SimpleStructureModule(nn.Module):\n\n    def __init__(self, d_model, nhead, \n                 dim_feedforward, pairwise_dimension, dropout=0.1,\n                 ):\n        super(SimpleStructureModule, self).__init__()\n        #self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n        self.self_attn = MultiHeadAttention(d_model, nhead, d_model//nhead, d_model//nhead, dropout=dropout)\n        #self.cross_attn = MultiHeadAttention(d_model, nhead, d_model//nhead, d_model//nhead, dropout=dropout)\n\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.pairwise2heads=nn.Linear(pairwise_dimension,nhead,bias=False)\n        self.pairwise_norm=nn.LayerNorm(pairwise_dimension)\n\n        #self.distance2heads=nn.Linear(1,nhead,bias=False)\n        #self.pairwise_norm=nn.LayerNorm(pairwise_dimension)\n\n        self.activation = nn.GELU()\n\n        \n    def custom(self, module):\n        def custom_forward(*inputs):\n            inputs = module(*inputs)\n            return inputs\n        return custom_forward\n\n    def forward(self, input):\n        tgt , src,  pairwise_features, pred_t, src_mask = input\n        \n        #src = src*src_mask.float().unsqueeze(-1)\n\n        pairwise_bias=self.pairwise2heads(self.pairwise_norm(pairwise_features)).permute(0,3,1,2)\n\n        \n\n\n        #print(pairwise_bias.shape,distance_bias.shape)\n\n        #pairwise_bias=pairwise_bias+distance_bias\n\n\n        res=tgt\n        tgt,attention_weights = self.self_attn(tgt, tgt, tgt, mask=pairwise_bias, src_mask=src_mask)\n        tgt = res + self.dropout1(tgt)\n        tgt = self.norm1(tgt)\n\n        # print(tgt.shape,src.shape)\n        # exit()\n\n        res=tgt\n        tgt = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n        tgt = res + self.dropout2(tgt)\n        tgt = self.norm2(tgt)\n\n\n        return tgt\n"}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":12024591,"sourceId":87793,"sourceType":"competition"}],"dockerImageVersionId":30919,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"dt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}